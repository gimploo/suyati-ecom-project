{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "rating = pd.read_csv('Ratings.csv')\n",
    "\n",
    "book = pd.read_csv('Books.csv')\n",
    "book_rating = pd.merge(rating, book, on='ISBN')\n",
    "cols = ['YearOfPublication', 'Publisher', 'BookAuthor', 'ImageURLS', 'ImageURLM', 'ImageURLL']\n",
    "book_rating.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "rating_count = (book_rating.\n",
    "     groupby(by = ['Book'])['BookRating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'BookRating': 'RatingCount_book'})\n",
    "     [['Book', 'RatingCount_book']]\n",
    "    )\n",
    "    \n",
    "threshold = 25\n",
    "rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
    "\n",
    "user_rating = pd.merge(rating_count, book_rating, left_on='Book', right_on='Book', how='left')\n",
    "\n",
    "user_count = (user_rating.\n",
    "     groupby(by = ['UserID'])['BookRating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'BookRating': 'RatingCount_user'})\n",
    "     [['UserID', 'RatingCount_user']]\n",
    "    )\n",
    "    \n",
    "threshold = 20\n",
    "user_count = user_count.query('RatingCount_user >= @threshold')\n",
    "\n",
    "combined = user_rating.merge(user_count, left_on = 'UserID', right_on = 'UserID', how = 'inner')\n",
    "\n",
    "print('Number of unique books: ', combined['Book'].nunique())\n",
    "print('Number of unique users: ', combined['UserID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined['BookRating'] = combined['BookRating'].values.astype(float)\n",
    "rating_scaled = pd.DataFrame(scaler.fit_transform(combined['BookRating'].values.reshape(-1,1)))\n",
    "combined['BookRating'] = rating_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop_duplicates(['UserID', 'Book'])\n",
    "user_book_matrix = combined.pivot(index='UserID', columns='Book', values='BookRating')\n",
    "user_book_matrix.fillna(0, inplace=True)\n",
    "users = user_book_matrix.index.tolist()\n",
    "books = user_book_matrix.columns.tolist()\n",
    "user_book_matrix = user_book_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = combined['Book'].nunique()\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "y_pred = decoder_op\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "pred_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 35\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "    user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        avg_cost = 0\n",
    "        for batch in user_book_matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "        print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    # print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "    print(epoch_accuracy.result())\n",
    "    preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
    "\n",
    "    pred_data = pred_data.append(pd.DataFrame(preds))\n",
    "\n",
    "    pred_data = pred_data.stack().reset_index(name='BookRating')\n",
    "    pred_data.columns = ['UserID', 'Book', 'BookRating']\n",
    "    pred_data['UserID'] = pred_data['UserID'].map(lambda value: users[value])\n",
    "    pred_data['Book'] = pred_data['Book'].map(lambda value: books[value])\n",
    "    \n",
    "    keys = ['UserID', 'Book']\n",
    "    index_1 = pred_data.set_index(keys).index\n",
    "    index_2 = combined.set_index(keys).index\n",
    "\n",
    "    top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
    "    top_ten_ranked = top_ten_ranked.sort_values(['UserID', 'Book'], ascending=[True, False])\n",
    "    top_ten_ranked = top_ten_ranked.groupby('UserID').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_ranked.loc[top_ten_ranked['UserID'] == 278582]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
